{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T07:26:12.295774Z",
     "iopub.status.busy": "2025-05-10T07:26:12.295260Z",
     "iopub.status.idle": "2025-05-10T07:27:41.962961Z",
     "shell.execute_reply": "2025-05-10T07:27:41.961987Z",
     "shell.execute_reply.started": "2025-05-10T07:26:12.295749Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q librosa numpy soundfile torchaudio praat-parselmouth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T07:27:41.965026Z",
     "iopub.status.busy": "2025-05-10T07:27:41.964772Z",
     "iopub.status.idle": "2025-05-10T07:27:48.927451Z",
     "shell.execute_reply": "2025-05-10T07:27:48.926856Z",
     "shell.execute_reply.started": "2025-05-10T07:27:41.965004Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt\n",
    "import parselmouth\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Optional, Dict, List\n",
    "import json\n",
    "\n",
    "class AudioPreprocessor:\n",
    "    def __init__(self, target_sr: int = 16000):\n",
    "        self.target_sr = target_sr\n",
    "        self.min_duration = 0.5  # minimum duration in seconds\n",
    "        self.max_duration = 10.0  # maximum duration in seconds\n",
    "        \n",
    "    def preprocess(self, audio_path: str) -> Tuple[np.ndarray, int]:\n",
    "        \"\"\"Enhanced audio preprocessing pipeline\"\"\"\n",
    "        # Load audio\n",
    "        y, sr = librosa.load(audio_path, sr=None)\n",
    "        \n",
    "        # Quality checks\n",
    "        if len(y) < self.min_duration * sr:\n",
    "            raise ValueError(f\"Audio too short: {len(y)/sr:.2f}s\")\n",
    "        if len(y) > self.max_duration * sr:\n",
    "            y = y[:int(self.max_duration * sr)]\n",
    "            \n",
    "        # Resample if needed\n",
    "        if sr != self.target_sr:\n",
    "            y = librosa.resample(y, orig_sr=sr, target_sr=self.target_sr)\n",
    "            \n",
    "        # Trim silence\n",
    "        y, _ = librosa.effects.trim(y, top_db=20, frame_length=2048, hop_length=512)\n",
    "        \n",
    "        # Apply pre-emphasis\n",
    "        y = librosa.effects.preemphasis(y)\n",
    "        \n",
    "        # Apply high-pass filter to remove DC offset\n",
    "        y = self._apply_highpass_filter(y, self.target_sr)\n",
    "        \n",
    "        # Normalize using LUFS\n",
    "        y = self._normalize_lufs(y, self.target_sr)\n",
    "        \n",
    "        return y, self.target_sr\n",
    "    \n",
    "    def _apply_highpass_filter(self, y: np.ndarray, sr: int, cutoff: float = 20.0) -> np.ndarray:\n",
    "        \"\"\"Apply high-pass filter to remove DC offset\"\"\"\n",
    "        try:\n",
    "            # Method 1: Using scipy's butter filter\n",
    "            nyquist = sr / 2\n",
    "            normal_cutoff = cutoff / nyquist\n",
    "            b, a = butter(4, normal_cutoff, btype='high', analog=False)\n",
    "            y_filtered = filtfilt(b, a, y)\n",
    "            return y_filtered\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: High-pass filter failed, using alternative method: {e}\")\n",
    "            try:\n",
    "                # Method 2: Using librosa's high-pass filter\n",
    "                y_filtered = librosa.effects.preemphasis(y, coef=0.97)\n",
    "                return y_filtered\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Alternative high-pass filter failed: {e}\")\n",
    "                # If both methods fail, return original signal\n",
    "                return y\n",
    "    \n",
    "    def _normalize_lufs(self, y: np.ndarray, sr: int, target_lufs: float = -23.0) -> np.ndarray:\n",
    "        \"\"\"Normalize audio to target LUFS level\"\"\"\n",
    "        try:\n",
    "            # Calculate current LUFS\n",
    "            current_lufs = self._calculate_lufs(y, sr)\n",
    "            \n",
    "            # Calculate gain adjustment\n",
    "            gain_db = target_lufs - current_lufs\n",
    "            gain_linear = 10 ** (gain_db / 20)\n",
    "            \n",
    "            # Apply gain\n",
    "            y_normalized = y * gain_linear\n",
    "            \n",
    "            # Prevent clipping\n",
    "            if np.max(np.abs(y_normalized)) > 0.99:\n",
    "                y_normalized = y_normalized * 0.99 / np.max(np.abs(y_normalized))\n",
    "                \n",
    "            return y_normalized\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: LUFS normalization failed: {e}\")\n",
    "            # Fallback to simple normalization\n",
    "            return librosa.util.normalize(y) * 0.95\n",
    "    \n",
    "    def _calculate_lufs(self, y: np.ndarray, sr: int) -> float:\n",
    "        \"\"\"Calculate LUFS (Loudness Units Full Scale)\"\"\"\n",
    "        try:\n",
    "            # Simplified LUFS calculation\n",
    "            rms = np.sqrt(np.mean(y ** 2))\n",
    "            lufs = 20 * np.log10(rms) + 0.691\n",
    "            return lufs\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: LUFS calculation failed: {e}\")\n",
    "            # Fallback to simple RMS\n",
    "            return 20 * np.log10(np.sqrt(np.mean(y ** 2)))\n",
    "            \n",
    "class FeatureExtractor:\n",
    "    def __init__(self, \n",
    "                 n_mels: int = 80,\n",
    "                 n_fft: int = 1024,\n",
    "                 hop_length: int = 256,\n",
    "                 win_length: int = 1024,\n",
    "                 fmin: float = 80.0,\n",
    "                 fmax: float = 7600.0,\n",
    "                 n_mfcc: int = 13):\n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = win_length\n",
    "        self.fmin = fmin\n",
    "        self.fmax = fmax\n",
    "        self.n_mfcc = n_mfcc\n",
    "        \n",
    "    def extract_features(self, y: np.ndarray, sr: int) -> dict:\n",
    "        \"\"\"Extract all required features\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # 1. Basic features\n",
    "        features['mel'] = self._extract_mel(y, sr)\n",
    "        features['prosody'] = self._extract_prosody(y, sr)\n",
    "        \n",
    "        # 2. Spectral features\n",
    "        features['spectral'] = self._extract_spectral_features(y, sr)\n",
    "        \n",
    "        # 3. Formant features\n",
    "        features['formants'] = self._extract_formants(y, sr)\n",
    "        \n",
    "        # 4. MFCC features\n",
    "        features['mfcc'] = self._extract_mfcc(y, sr)\n",
    "        \n",
    "        # 5. Voice quality features\n",
    "        features['voice_quality'] = self._extract_voice_quality(y, sr)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_mel(self, y: np.ndarray, sr: int) -> np.ndarray:\n",
    "        \"\"\"Extract mel spectrogram with enhanced parameters\"\"\"\n",
    "        mel = librosa.feature.melspectrogram(\n",
    "            y=y,\n",
    "            sr=sr,\n",
    "            n_mels=self.n_mels,\n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.hop_length,\n",
    "            win_length=self.win_length,\n",
    "            fmin=self.fmin,\n",
    "            fmax=self.fmax,\n",
    "            power=1.0\n",
    "        )\n",
    "        \n",
    "        # Convert to log scale\n",
    "        mel = librosa.power_to_db(mel, ref=1.0)\n",
    "        \n",
    "        # Normalize\n",
    "        mel = (mel - mel.mean()) / (mel.std() + 1e-8)\n",
    "        \n",
    "        return mel\n",
    "    \n",
    "    def _extract_prosody(self, y: np.ndarray, sr: int) -> np.ndarray:\n",
    "        \"\"\"Extract prosody features using librosa.pyin\"\"\"\n",
    "        # Extract F0 with proper voicing detection\n",
    "        f0, voiced_flag, _ = librosa.pyin(\n",
    "            y, \n",
    "            fmin=80, \n",
    "            fmax=400, \n",
    "            sr=sr,\n",
    "            frame_length=self.n_fft, \n",
    "            hop_length=self.hop_length\n",
    "        )\n",
    "        f0[~voiced_flag] = 0  # Set unvoiced to 0\n",
    "        \n",
    "        # Energy with same params as mel\n",
    "        energy = librosa.feature.rms(\n",
    "            y=y, \n",
    "            frame_length=self.n_fft, \n",
    "            hop_length=self.hop_length\n",
    "        ).squeeze()\n",
    "        \n",
    "        # Duration\n",
    "        duration = np.linspace(0, 1, len(f0))\n",
    "        \n",
    "        # Normalize\n",
    "        f0 = (f0 - np.mean(f0)) / (np.std(f0) + 1e-6)\n",
    "        energy = (energy - np.mean(energy)) / (np.std(energy) + 1e-6)\n",
    "        \n",
    "        return np.stack([f0, energy, duration], axis=1)\n",
    "    \n",
    "    def _extract_spectral_features(self, y: np.ndarray, sr: int) -> dict:\n",
    "        \"\"\"Extract spectral features\"\"\"\n",
    "        # Spectral Centroid\n",
    "        centroid = librosa.feature.spectral_centroid(\n",
    "            y=y, sr=sr, n_fft=self.n_fft, hop_length=self.hop_length\n",
    "        )[0]\n",
    "        \n",
    "        # Spectral Bandwidth\n",
    "        bandwidth = librosa.feature.spectral_bandwidth(\n",
    "            y=y, sr=sr, n_fft=self.n_fft, hop_length=self.hop_length\n",
    "        )[0]\n",
    "        \n",
    "        # Spectral Contrast\n",
    "        contrast = librosa.feature.spectral_contrast(\n",
    "            y=y, sr=sr, n_fft=self.n_fft, hop_length=self.hop_length\n",
    "        )\n",
    "        \n",
    "        # Spectral Rolloff\n",
    "        rolloff = librosa.feature.spectral_rolloff(\n",
    "            y=y, sr=sr, n_fft=self.n_fft, hop_length=self.hop_length\n",
    "        )[0]\n",
    "        \n",
    "        # Normalize features\n",
    "        centroid = (centroid - np.mean(centroid)) / (np.std(centroid) + 1e-6)\n",
    "        bandwidth = (bandwidth - np.mean(bandwidth)) / (np.std(bandwidth) + 1e-6)\n",
    "        contrast = (contrast - np.mean(contrast)) / (np.std(contrast) + 1e-6)\n",
    "        rolloff = (rolloff - np.mean(rolloff)) / (np.std(rolloff) + 1e-6)\n",
    "        \n",
    "        return {\n",
    "            'centroid': centroid,    # Brightness of sound\n",
    "            'bandwidth': bandwidth,  # Spread of frequencies\n",
    "            'contrast': contrast,    # Peak vs. valley differences\n",
    "            'rolloff': rolloff      # Frequency distribution\n",
    "        }\n",
    "    \n",
    "    def _extract_formants(self, y: np.ndarray, sr: int) -> np.ndarray:\n",
    "        \"\"\"Extract formant frequencies (F1, F2, F3)\"\"\"\n",
    "        try:\n",
    "            # Use Praat through parselmouth\n",
    "            snd = parselmouth.Sound(y, sr)\n",
    "            formants = snd.to_formant_burg()\n",
    "            \n",
    "            # Get time points for formant extraction\n",
    "            # Use the same hop_length as other features for consistency\n",
    "            time_points = np.arange(0, len(y)/sr, self.hop_length/sr)\n",
    "            \n",
    "            # Initialize arrays for formants\n",
    "            f1 = np.zeros(len(time_points))\n",
    "            f2 = np.zeros(len(time_points))\n",
    "            f3 = np.zeros(len(time_points))\n",
    "            \n",
    "            # Extract formants at each time point\n",
    "            for i, t in enumerate(time_points):\n",
    "                try:\n",
    "                    f1[i] = formants.get_value_at_time(1, t)\n",
    "                    f2[i] = formants.get_value_at_time(2, t)\n",
    "                    f3[i] = formants.get_value_at_time(3, t)\n",
    "                except:\n",
    "                    # If formant extraction fails at this point, use previous values\n",
    "                    if i > 0:\n",
    "                        f1[i] = f1[i-1]\n",
    "                        f2[i] = f2[i-1]\n",
    "                        f3[i] = f3[i-1]\n",
    "                    else:\n",
    "                        f1[i] = 0\n",
    "                        f2[i] = 0\n",
    "                        f3[i] = 0\n",
    "            \n",
    "            # Handle NaN values\n",
    "            f1 = np.nan_to_num(f1, nan=0.0)\n",
    "            f2 = np.nan_to_num(f2, nan=0.0)\n",
    "            f3 = np.nan_to_num(f3, nan=0.0)\n",
    "            \n",
    "            # Normalize\n",
    "            f1 = (f1 - np.mean(f1)) / (np.std(f1) + 1e-6)\n",
    "            f2 = (f2 - np.mean(f2)) / (np.std(f2) + 1e-6)\n",
    "            f3 = (f3 - np.mean(f3)) / (np.std(f3) + 1e-6)\n",
    "            \n",
    "            return np.stack([f1, f2, f3], axis=1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting formants: {e}\")\n",
    "            # Return zero array if formant extraction fails\n",
    "            return np.zeros((len(y)//self.hop_length, 3))\n",
    "    \n",
    "    def _extract_mfcc(self, y: np.ndarray, sr: int) -> np.ndarray:\n",
    "        \"\"\"Extract MFCC features\"\"\"\n",
    "        # Extract base MFCC\n",
    "        mfcc = librosa.feature.mfcc(\n",
    "            y=y, \n",
    "            sr=sr,\n",
    "            n_mfcc=self.n_mfcc,\n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.hop_length\n",
    "        )\n",
    "        \n",
    "        # Add delta and delta-delta features\n",
    "        delta = librosa.feature.delta(mfcc)\n",
    "        delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "        \n",
    "        # Normalize\n",
    "        mfcc = (mfcc - np.mean(mfcc)) / (np.std(mfcc) + 1e-6)\n",
    "        delta = (delta - np.mean(delta)) / (np.std(delta) + 1e-6)\n",
    "        delta2 = (delta2 - np.mean(delta2)) / (np.std(delta2) + 1e-6)\n",
    "        \n",
    "        return np.concatenate([mfcc, delta, delta2], axis=0)\n",
    "    \n",
    "    def _extract_voice_quality(self, y: np.ndarray, sr: int) -> dict:\n",
    "        \"\"\"Extract voice quality features\"\"\"\n",
    "        # Jitter (pitch period variation)\n",
    "        jitter = librosa.feature.zero_crossing_rate(y)\n",
    "        \n",
    "        # Shimmer (amplitude variation)\n",
    "        shimmer = np.diff(np.abs(y))\n",
    "        # Pad shimmer to match other features\n",
    "        shimmer = np.pad(shimmer, (0, 1), mode='edge')\n",
    "        \n",
    "        # Harmonics-to-Noise Ratio (HNR)\n",
    "        try:\n",
    "            hnr = librosa.effects.harmonic(y)\n",
    "            hnr = np.abs(hnr)  # Take magnitude\n",
    "        except:\n",
    "            hnr = np.zeros_like(y)\n",
    "        \n",
    "        # Resample to match other features\n",
    "        target_length = len(y) // self.hop_length\n",
    "        jitter = librosa.resample(jitter, orig_sr=sr/self.hop_length, target_sr=target_length)\n",
    "        shimmer = librosa.resample(shimmer, orig_sr=sr, target_sr=target_length)\n",
    "        hnr = librosa.resample(hnr, orig_sr=sr, target_sr=target_length)\n",
    "        \n",
    "        # Normalize\n",
    "        jitter = (jitter - np.mean(jitter)) / (np.std(jitter) + 1e-6)\n",
    "        shimmer = (shimmer - np.mean(shimmer)) / (np.std(shimmer) + 1e-6)\n",
    "        hnr = (hnr - np.mean(hnr)) / (np.std(hnr) + 1e-6)\n",
    "        \n",
    "        return {\n",
    "            'jitter': jitter,\n",
    "            'shimmer': shimmer,\n",
    "            'hnr': hnr\n",
    "        }\n",
    "\n",
    "class FeatureProcessor:\n",
    "    def __init__(self, \n",
    "                 input_root: str,\n",
    "                 output_root: str,\n",
    "                 target_sr: int = 16000):\n",
    "        self.input_root = input_root\n",
    "        self.output_root = output_root\n",
    "        self.preprocessor = AudioPreprocessor(target_sr)\n",
    "        self.feature_extractor = FeatureExtractor()\n",
    "        \n",
    "        # Define accents and speakers\n",
    "        self.accents = {\n",
    "            \"hindi\": [\"ASI\", \"RRBI\", \"SVBI\", \"TNI\"],\n",
    "            \"spanish\": [\"EBVS\", \"ERMS\", \"MBMPS\", \"NJS\"]\n",
    "        }\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(output_root, exist_ok=True)\n",
    "        \n",
    "    def process_dataset(self):\n",
    "        \"\"\"Process all audio files in the dataset\"\"\"\n",
    "        # Create metadata dictionary\n",
    "        metadata = {\n",
    "            'accents': self.accents,\n",
    "            'processed_files': []\n",
    "        }\n",
    "        \n",
    "        # Process each accent\n",
    "        for accent, speakers in self.accents.items():\n",
    "            print(f\"\\nProcessing {accent} accent...\")\n",
    "            \n",
    "            for speaker in speakers:\n",
    "                print(f\"\\nProcessing speaker: {speaker}\")\n",
    "                \n",
    "                # Create speaker directory\n",
    "                speaker_dir = os.path.join(self.output_root, speaker)\n",
    "                os.makedirs(speaker_dir, exist_ok=True)\n",
    "                \n",
    "                # Get all wav files for this speaker\n",
    "                wav_path = os.path.join(self.input_root, speaker, speaker, \"wav\")\n",
    "                if not os.path.exists(wav_path):\n",
    "                    print(f\"Warning: No wav directory found for speaker {speaker}\")\n",
    "                    continue\n",
    "                    \n",
    "                wav_files = glob.glob(os.path.join(wav_path, \"*.wav\"))\n",
    "                \n",
    "                # Process each wav file\n",
    "                for wav_file in tqdm(wav_files, desc=f\"Processing {speaker}\"):\n",
    "                    try:\n",
    "                        # Get utterance ID\n",
    "                        utt_id = os.path.basename(wav_file).replace(\".wav\", \"\")\n",
    "                        \n",
    "                        # Preprocess audio\n",
    "                        y, sr = self.preprocessor.preprocess(wav_file)\n",
    "                        \n",
    "                        # Extract features\n",
    "                        features = self.feature_extractor.extract_features(y, sr)\n",
    "                        \n",
    "                        # Save features\n",
    "                        self._save_features(speaker_dir, utt_id, features)\n",
    "                        \n",
    "                        # Update metadata\n",
    "                        metadata['processed_files'].append({\n",
    "                            'speaker': speaker,\n",
    "                            'accent': accent,\n",
    "                            'utt_id': utt_id,\n",
    "                            'duration': len(y) / sr\n",
    "                        })\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {wav_file}: {e}\")\n",
    "                        continue\n",
    "        \n",
    "        # Save metadata\n",
    "        with open(os.path.join(self.output_root, 'metadata.json'), 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "            \n",
    "        print(\"\\nFeature extraction completed!\")\n",
    "        print(f\"Processed files: {len(metadata['processed_files'])}\")\n",
    "        \n",
    "    def _save_features(self, speaker_dir: str, utt_id: str, features: dict):\n",
    "        \"\"\"Save extracted features\"\"\"\n",
    "        # Save each feature type\n",
    "        for feature_name, feature_data in features.items():\n",
    "            if isinstance(feature_data, dict):\n",
    "                # Handle nested features (like spectral features)\n",
    "                for sub_feature_name, sub_feature_data in feature_data.items():\n",
    "                    np.save(\n",
    "                        os.path.join(speaker_dir, f\"{utt_id}_{feature_name}_{sub_feature_name}.npy\"),\n",
    "                        sub_feature_data\n",
    "                    )\n",
    "            else:\n",
    "                # Handle direct features\n",
    "                np.save(\n",
    "                    os.path.join(speaker_dir, f\"{utt_id}_{feature_name}.npy\"),\n",
    "                    feature_data\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T07:27:48.928372Z",
     "iopub.status.busy": "2025-05-10T07:27:48.928096Z",
     "iopub.status.idle": "2025-05-10T08:51:37.709527Z",
     "shell.execute_reply": "2025-05-10T08:51:37.708701Z",
     "shell.execute_reply.started": "2025-05-10T07:27:48.928356Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing hindi accent...\n",
      "\n",
      "Processing speaker: ASI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ASI: 100%|██████████| 1131/1131 [09:23<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing speaker: RRBI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing RRBI: 100%|██████████| 1130/1130 [10:38<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing speaker: SVBI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVBI: 100%|██████████| 1132/1132 [09:08<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing speaker: TNI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing TNI: 100%|██████████| 1131/1131 [09:39<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing spanish accent...\n",
      "\n",
      "Processing speaker: EBVS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing EBVS: 100%|██████████| 1007/1007 [10:47<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing speaker: ERMS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ERMS: 100%|██████████| 1132/1132 [12:12<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing speaker: MBMPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MBMPS: 100%|██████████| 1132/1132 [12:40<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing speaker: NJS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing NJS: 100%|██████████| 1131/1131 [09:18<00:00,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature extraction completed!\n",
      "Processed files: 8926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Set paths\n",
    "    L2_ARCTIC_ROOT = \"/kaggle/input/l2-arctic-data\"\n",
    "    PROCESSED_DIR = \"/kaggle/working/processed_features\"\n",
    "    \n",
    "    # Create processor\n",
    "    processor = FeatureProcessor(\n",
    "        input_root=L2_ARCTIC_ROOT,\n",
    "        output_root=PROCESSED_DIR\n",
    "    )\n",
    "    \n",
    "    # Process dataset\n",
    "    processor.process_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T15:22:19.325509Z",
     "iopub.status.busy": "2025-05-10T15:22:19.324800Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset created successfully with 98186 samples\n",
      "First sample mel shape: torch.Size([128, 128])\n",
      "First sample prosody shape: torch.Size([128, 128])\n",
      "First sample accent: hindi\n",
      "First sample label index: 0\n",
      "Label to index mapping: {'hindi': 0, 'spanish': 1}\n",
      "Split dataset: 88367 training samples, 9819 validation samples\n",
      "Model created successfully\n",
      "Model parameters: 4040899\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 2762/2762 [08:29<00:00,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n",
      "Train Loss: 1.2823\n",
      "Train Triplet: 0.9998\n",
      "Train Recon: 0.0145\n",
      "Train Contrast: 2.8098\n",
      "Val Loss: 1.2802\n",
      "Val Triplet: 1.0002\n",
      "Val Recon: 0.0118\n",
      "Val Contrast: 2.7876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50:  22%|██▏       | 594/2762 [01:49<06:39,  5.42it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "\n",
    "class AccentEmbeddingDataset(Dataset):\n",
    "    def __init__(self, feature_root, accents, target_size=(128, 128)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feature_root (str): Root directory containing the processed features\n",
    "            accents (dict): Dictionary mapping accent groups to their codes\n",
    "            target_size (tuple): Target size for mel spectrograms (time_steps, n_mels)\n",
    "        \"\"\"\n",
    "        self.feature_root = feature_root\n",
    "        self.accents = accents\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        # Get all feature files and their labels\n",
    "        self.feature_files = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Walk through the directory structure\n",
    "        for accent_group, accent_codes in accents.items():\n",
    "            for accent_code in accent_codes:\n",
    "                accent_dir = os.path.join(feature_root, accent_code)\n",
    "                if os.path.exists(accent_dir):\n",
    "                    for file in os.listdir(accent_dir):\n",
    "                        if file.endswith('.npy'):\n",
    "                            self.feature_files.append(os.path.join(accent_dir, file))\n",
    "                            self.labels.append(accent_group)\n",
    "        \n",
    "        # Create label to index mapping\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(sorted(set(self.labels)))}\n",
    "        self.idx_to_label = {idx: label for label, idx in self.label_to_idx.items()}\n",
    "        \n",
    "        # Validate at least one file exists\n",
    "        if len(self.feature_files) == 0:\n",
    "            raise ValueError(f\"No .npy files found in the specified directories: {feature_root}\")\n",
    "            \n",
    "        # Check the format of the first feature file to determine how to load data\n",
    "        sample_file = self.feature_files[0]\n",
    "        sample_features = np.load(sample_file, allow_pickle=True)\n",
    "        \n",
    "        if isinstance(sample_features, np.ndarray) and sample_features.dtype == np.dtype('object'):\n",
    "            # Array of arrays\n",
    "            self.is_array_of_arrays = True\n",
    "        else:\n",
    "            # Single array, need to reshape\n",
    "            self.is_array_of_arrays = False\n",
    "            # Determine if it's a structured array with named fields\n",
    "            self.has_structured_fields = hasattr(sample_features, 'dtype') and sample_features.dtype.names is not None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.feature_files)\n",
    "    \n",
    "    def pad_or_truncate(self, tensor, target_size):\n",
    "        \"\"\"Pad or truncate tensor to target size\"\"\"\n",
    "        # Convert to tensor if not already\n",
    "        if not isinstance(tensor, torch.Tensor):\n",
    "            tensor = torch.tensor(tensor, dtype=torch.float32)\n",
    "            \n",
    "        # Ensure tensor is 2D\n",
    "        if len(tensor.shape) == 1:\n",
    "            tensor = tensor.unsqueeze(0)  # Add channel dimension\n",
    "        \n",
    "        current_size = tensor.shape\n",
    "        \n",
    "        # Handle time dimension (first dimension)\n",
    "        if current_size[0] < target_size[0]:\n",
    "            # Pad time dimension\n",
    "            pad_size = target_size[0] - current_size[0]\n",
    "            tensor = F.pad(tensor, (0, 0, 0, pad_size))\n",
    "        elif current_size[0] > target_size[0]:\n",
    "            # Truncate time dimension\n",
    "            tensor = tensor[:target_size[0], :]\n",
    "            \n",
    "        # Handle frequency dimension (second dimension)\n",
    "        if current_size[1] < target_size[1]:\n",
    "            # Pad frequency dimension\n",
    "            pad_size = target_size[1] - current_size[1]\n",
    "            tensor = F.pad(tensor, (0, pad_size))\n",
    "        elif current_size[1] > target_size[1]:\n",
    "            # Truncate frequency dimension\n",
    "            tensor = tensor[:, :target_size[1]]\n",
    "            \n",
    "        return tensor\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature_path = self.feature_files[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load features\n",
    "        features = np.load(feature_path, allow_pickle=True)\n",
    "        \n",
    "        # Handle different feature formats\n",
    "        if self.is_array_of_arrays:\n",
    "            # Features are an array of arrays\n",
    "            mel_spec = torch.tensor(features[0], dtype=torch.float32)\n",
    "            prosody = torch.tensor(features[1], dtype=torch.float32)\n",
    "            mfcc = torch.tensor(features[2], dtype=torch.float32)\n",
    "            chroma = torch.tensor(features[3], dtype=torch.float32)\n",
    "            spectral_contrast = torch.tensor(features[4], dtype=torch.float32)\n",
    "            tonnetz = torch.tensor(features[5], dtype=torch.float32)\n",
    "        elif self.has_structured_fields:\n",
    "            # Features are a structured array with named fields\n",
    "            mel_spec = torch.tensor(features['mel_spec'], dtype=torch.float32)\n",
    "            prosody = torch.tensor(features['prosody'], dtype=torch.float32)\n",
    "            mfcc = torch.tensor(features['mfcc'], dtype=torch.float32)\n",
    "            chroma = torch.tensor(features['chroma'], dtype=torch.float32)\n",
    "            spectral_contrast = torch.tensor(features['spectral_contrast'], dtype=torch.float32)\n",
    "            tonnetz = torch.tensor(features['tonnetz'], dtype=torch.float32)\n",
    "        else:\n",
    "            # Assume features is a single array that needs to be split\n",
    "            # This is a simplification - adjust based on your actual data format\n",
    "            feature_size = features.shape[0] // 6\n",
    "            mel_spec = torch.tensor(features[:feature_size], dtype=torch.float32)\n",
    "            prosody = torch.tensor(features[feature_size:2*feature_size], dtype=torch.float32)\n",
    "            mfcc = torch.tensor(features[2*feature_size:3*feature_size], dtype=torch.float32)\n",
    "            chroma = torch.tensor(features[3*feature_size:4*feature_size], dtype=torch.float32)\n",
    "            spectral_contrast = torch.tensor(features[4*feature_size:5*feature_size], dtype=torch.float32)\n",
    "            tonnetz = torch.tensor(features[5*feature_size:], dtype=torch.float32)\n",
    "        \n",
    "        # Process mel spectrogram\n",
    "        mel_spec = self.pad_or_truncate(mel_spec, self.target_size)\n",
    "        \n",
    "        # Process prosody features\n",
    "        prosody = self.pad_or_truncate(prosody, self.target_size)\n",
    "        \n",
    "        # Process spectral features\n",
    "        spectral = {\n",
    "            'mfcc': self.pad_or_truncate(mfcc, self.target_size),\n",
    "            'chroma': self.pad_or_truncate(chroma, self.target_size),\n",
    "            'spectral_contrast': self.pad_or_truncate(spectral_contrast, self.target_size),\n",
    "            'tonnetz': self.pad_or_truncate(tonnetz, self.target_size)\n",
    "        }\n",
    "        \n",
    "        # Convert label to index\n",
    "        label_idx = self.label_to_idx[label]\n",
    "        \n",
    "        return {\n",
    "            'mel': mel_spec,\n",
    "            'prosody': prosody,\n",
    "            'spectral': spectral,\n",
    "            'label': label_idx,\n",
    "            'accent': label,\n",
    "            'feature_path': feature_path\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle the batch creation\n",
    "    \"\"\"\n",
    "    # Stack mel spectrograms\n",
    "    mel = torch.stack([item['mel'] for item in batch])\n",
    "    \n",
    "    # Stack prosody features\n",
    "    prosody = torch.stack([item['prosody'] for item in batch])\n",
    "    \n",
    "    # Stack spectral features\n",
    "    spectral = {\n",
    "        'mfcc': torch.stack([item['spectral']['mfcc'] for item in batch]),\n",
    "        'chroma': torch.stack([item['spectral']['chroma'] for item in batch]),\n",
    "        'spectral_contrast': torch.stack([item['spectral']['spectral_contrast'] for item in batch]),\n",
    "        'tonnetz': torch.stack([item['spectral']['tonnetz'] for item in batch])\n",
    "    }\n",
    "    \n",
    "    # Stack labels (numeric indices)\n",
    "    labels = torch.tensor([item['label'] for item in batch])\n",
    "    \n",
    "    # Get other information\n",
    "    accents = [item['accent'] for item in batch]\n",
    "    feature_paths = [item['feature_path'] for item in batch]\n",
    "    \n",
    "    return {\n",
    "        'mel': mel,\n",
    "        'prosody': prosody,\n",
    "        'spectral': spectral,\n",
    "        'label': labels,\n",
    "        'accent': accents,\n",
    "        'feature_path': feature_paths\n",
    "    }\n",
    "\n",
    "class EnhancedAccentEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 mel_dim: int = 128,\n",
    "                 prosody_dim: int = 128,\n",
    "                 spectral_dim: int = 128,\n",
    "                 hidden_dim: int = 256,\n",
    "                 emb_dim: int = 64,\n",
    "                 num_heads: int = 8,\n",
    "                 num_layers: int = 3,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature-specific encoders with residual connections\n",
    "        self.mel_encoder = ResidualBlock(mel_dim, hidden_dim)\n",
    "        self.prosody_encoder = ResidualBlock(prosody_dim, hidden_dim)\n",
    "        \n",
    "        # Combined spectral features encoder (mfcc + chroma + spectral_contrast + tonnetz)\n",
    "        # We'll use a separate encoder for each feature type and then combine them\n",
    "        self.mfcc_encoder = nn.Conv1d(mel_dim, hidden_dim // 4, kernel_size=3, padding=1)\n",
    "        self.chroma_encoder = nn.Conv1d(mel_dim, hidden_dim // 4, kernel_size=3, padding=1)\n",
    "        self.spectral_contrast_encoder = nn.Conv1d(mel_dim, hidden_dim // 4, kernel_size=3, padding=1)\n",
    "        self.tonnetz_encoder = nn.Conv1d(mel_dim, hidden_dim // 4, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Feature combination and residual block for spectral features\n",
    "        self.spectral_combiner = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=1)\n",
    "        self.spectral_residual = ResidualBlock(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Learnable feature weights\n",
    "        self.feature_weights = nn.Parameter(torch.ones(3))\n",
    "        \n",
    "        # Transformer encoder for temporal modeling\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(hidden_dim)\n",
    "        \n",
    "        # Feature fusion with attention\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 3, hidden_dim * 2),\n",
    "            nn.LayerNorm(hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, emb_dim)\n",
    "        )\n",
    "        \n",
    "        # Reconstruction heads for multi-task learning\n",
    "        self.mel_head = nn.Linear(emb_dim, mel_dim)\n",
    "        self.prosody_head = nn.Linear(emb_dim, prosody_dim)\n",
    "        self.spectral_head = nn.Linear(emb_dim, spectral_dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, mel, prosody, spectral):\n",
    "        # Encode mel and prosody features with residual connections\n",
    "        mel_feat = self.mel_encoder(mel)  # [B, hidden_dim, T]\n",
    "        prosody_feat = self.prosody_encoder(prosody)  # [B, hidden_dim, T]\n",
    "        \n",
    "        # Process spectral features\n",
    "        mfcc_feat = F.relu(self.mfcc_encoder(spectral['mfcc']))\n",
    "        chroma_feat = F.relu(self.chroma_encoder(spectral['chroma']))\n",
    "        contrast_feat = F.relu(self.spectral_contrast_encoder(spectral['spectral_contrast']))\n",
    "        tonnetz_feat = F.relu(self.tonnetz_encoder(spectral['tonnetz']))\n",
    "        \n",
    "        # Combine spectral features\n",
    "        spectral_combined = torch.cat([mfcc_feat, chroma_feat, contrast_feat, tonnetz_feat], dim=1)\n",
    "        spectral_feat = self.spectral_combiner(spectral_combined)\n",
    "        spectral_feat = self.spectral_residual(spectral_feat)\n",
    "        \n",
    "        # Apply learned feature weights\n",
    "        mel_feat = mel_feat * self.feature_weights[0]\n",
    "        prosody_feat = prosody_feat * self.feature_weights[1]\n",
    "        spectral_feat = spectral_feat * self.feature_weights[2]\n",
    "        \n",
    "        # Prepare for transformer\n",
    "        mel_feat = mel_feat.transpose(1, 2)  # [B, T, hidden_dim]\n",
    "        prosody_feat = prosody_feat.transpose(1, 2)\n",
    "        spectral_feat = spectral_feat.transpose(1, 2)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        mel_feat = self.pos_encoder(mel_feat)\n",
    "        prosody_feat = self.pos_encoder(prosody_feat)\n",
    "        spectral_feat = self.pos_encoder(spectral_feat)\n",
    "        \n",
    "        # Apply transformer\n",
    "        mel_feat = self.transformer(mel_feat)\n",
    "        prosody_feat = self.transformer(prosody_feat)\n",
    "        spectral_feat = self.transformer(spectral_feat)\n",
    "        \n",
    "        # Global average pooling\n",
    "        mel_feat = torch.mean(mel_feat, dim=1)\n",
    "        prosody_feat = torch.mean(prosody_feat, dim=1)\n",
    "        spectral_feat = torch.mean(spectral_feat, dim=1)\n",
    "        \n",
    "        # Combine features\n",
    "        combined = torch.cat([mel_feat, prosody_feat, spectral_feat], dim=1)\n",
    "        \n",
    "        # Generate embedding\n",
    "        embedding = self.fusion(combined)\n",
    "        embedding = F.normalize(embedding, p=2, dim=1)\n",
    "        \n",
    "        # Reconstruction\n",
    "        mel_recon = self.mel_head(embedding)\n",
    "        prosody_recon = self.prosody_head(embedding)\n",
    "        spectral_recon = self.spectral_head(embedding)\n",
    "        \n",
    "        return embedding, mel_recon, prosody_recon, spectral_recon\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, 1),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += residual\n",
    "        return F.relu(out)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, embeddings, labels):\n",
    "        \"\"\"\n",
    "        Compute contrastive loss from embeddings and labels\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Tensor of shape [batch_size, embedding_dimension]\n",
    "            labels: Tensor of numeric indices for each embedding\n",
    "            \n",
    "        Returns:\n",
    "            Contrastive loss value\n",
    "        \"\"\"\n",
    "        batch_size = embeddings.size(0)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        similarity_matrix = torch.matmul(embeddings, embeddings.T) / self.temperature\n",
    "        \n",
    "        # Create mask for positive pairs - same accent = positive pair\n",
    "        # Make sure labels is a tensor of integers\n",
    "        if not isinstance(labels, torch.Tensor):\n",
    "            labels = torch.tensor(labels, device=embeddings.device)\n",
    "            \n",
    "        mask = labels.unsqueeze(0) == labels.unsqueeze(1)\n",
    "        \n",
    "        # Remove self-similarity from positive pair mask\n",
    "        mask.fill_diagonal_(False)\n",
    "        \n",
    "        # Create negative mask - different accent = negative pair\n",
    "        negative_mask = ~mask\n",
    "        \n",
    "        # Compute log_prob for positive pairs\n",
    "        exp_sim = torch.exp(similarity_matrix)\n",
    "        \n",
    "        # Sum for normalization constant, excluding self-similarity\n",
    "        log_prob = similarity_matrix - torch.log(\n",
    "            (exp_sim * negative_mask.float()).sum(dim=1, keepdim=True) + 1e-8\n",
    "        )\n",
    "        \n",
    "        # Compute mean of positive pairs log_prob\n",
    "        mean_log_prob = (mask.float() * log_prob).sum(dim=1) / (mask.float().sum(dim=1) + 1e-8)\n",
    "        \n",
    "        # Loss is negative mean of log probabilities\n",
    "        loss = -mean_log_prob.mean()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "def train_accent_encoder(model, train_loader, val_loader, num_epochs, device):\n",
    "    \"\"\"Train the accent encoder model\"\"\"\n",
    "    \n",
    "    # Initialize optimizers and schedulers\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-6)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=1e-3, epochs=num_epochs, steps_per_epoch=len(train_loader)\n",
    "    )\n",
    "    \n",
    "    # Loss functions\n",
    "    triplet_loss_fn = nn.TripletMarginLoss(margin=1.0)\n",
    "    reconstruction_loss_fn = nn.MSELoss()\n",
    "    contrastive_loss_fn = ContrastiveLoss(temperature=0.07)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_triplet = 0\n",
    "        train_recon = 0\n",
    "        train_contrast = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            # Move data to device\n",
    "            mel = batch['mel'].to(device)\n",
    "            prosody = batch['prosody'].to(device)\n",
    "            \n",
    "            # Handle spectral features\n",
    "            spectral = {}\n",
    "            for key, value in batch['spectral'].items():\n",
    "                spectral[key] = value.to(device)\n",
    "            \n",
    "            # Get numeric labels - already tensor from DataLoader\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            embedding, mel_recon, prosody_recon, spectral_recon = model(mel, prosody, spectral)\n",
    "            \n",
    "            # Calculate triplet loss\n",
    "            triplet = torch.tensor(0.0, device=device)\n",
    "            if embedding.size(0) >= 3:  # Need at least 3 samples for triplet loss\n",
    "                # Find positive and negative samples for each anchor based on numeric labels\n",
    "                for i in range(len(embedding)):\n",
    "                    # Find positive examples (same label)\n",
    "                    positive_indices = [j for j in range(len(labels)) \n",
    "                                      if j != i and labels[j].item() == labels[i].item()]\n",
    "                    # Find negative examples (different label)\n",
    "                    negative_indices = [j for j in range(len(labels)) \n",
    "                                      if labels[j].item() != labels[i].item()]\n",
    "                    \n",
    "                    if positive_indices and negative_indices:  # Make sure we have valid pairs\n",
    "                        pos_idx = np.random.choice(positive_indices)\n",
    "                        neg_idx = np.random.choice(negative_indices)\n",
    "                        \n",
    "                        anchor = embedding[i].unsqueeze(0)\n",
    "                        positive = embedding[pos_idx].unsqueeze(0)\n",
    "                        negative = embedding[neg_idx].unsqueeze(0)\n",
    "                        \n",
    "                        triplet += triplet_loss_fn(anchor, positive, negative)\n",
    "                \n",
    "                # Average triplet loss\n",
    "                if len(embedding) > 0:\n",
    "                    triplet = triplet / len(embedding)\n",
    "            \n",
    "            # Reconstruction loss\n",
    "            recon = reconstruction_loss_fn(mel_recon, mel.mean(dim=2)) + \\\n",
    "                   reconstruction_loss_fn(prosody_recon, prosody.mean(dim=2)) + \\\n",
    "                   reconstruction_loss_fn(spectral_recon, spectral['mfcc'].mean(dim=2))  # Using MFCC as representative\n",
    "            \n",
    "            # Contrastive loss - use numeric labels\n",
    "            contrast = contrastive_loss_fn(embedding, labels)\n",
    "            \n",
    "            # Combined loss - weighted sum\n",
    "            loss = triplet + 0.1 * recon + 0.1 * contrast\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            total_loss += loss.item()\n",
    "            train_triplet += triplet.item()\n",
    "            train_recon += recon.item()\n",
    "            train_contrast += contrast.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_triplet = 0\n",
    "        val_recon = 0\n",
    "        val_contrast = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Move data to device\n",
    "                mel = batch['mel'].to(device)\n",
    "                prosody = batch['prosody'].to(device)\n",
    "                \n",
    "                # Handle spectral features\n",
    "                spectral = {}\n",
    "                for key, value in batch['spectral'].items():\n",
    "                    spectral[key] = value.to(device)\n",
    "                \n",
    "                # Get numeric labels\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                embedding, mel_recon, prosody_recon, spectral_recon = model(mel, prosody, spectral)\n",
    "                \n",
    "                # Calculate same losses as in training\n",
    "                triplet = torch.tensor(0.0, device=device)\n",
    "                if embedding.size(0) >= 3:\n",
    "                    for i in range(len(embedding)):\n",
    "                        positive_indices = [j for j in range(len(labels)) \n",
    "                                          if j != i and labels[j].item() == labels[i].item()]\n",
    "                        negative_indices = [j for j in range(len(labels)) \n",
    "                                          if labels[j].item() != labels[i].item()]\n",
    "                        \n",
    "                        if positive_indices and negative_indices:\n",
    "                            pos_idx = np.random.choice(positive_indices)\n",
    "                            neg_idx = np.random.choice(negative_indices)\n",
    "                            \n",
    "                            anchor = embedding[i].unsqueeze(0)\n",
    "                            positive = embedding[pos_idx].unsqueeze(0)\n",
    "                            negative = embedding[neg_idx].unsqueeze(0)\n",
    "                            \n",
    "                            triplet += triplet_loss_fn(anchor, positive, negative)\n",
    "                    \n",
    "                    if len(embedding) > 0:\n",
    "                        triplet = triplet / len(embedding)\n",
    "                \n",
    "                recon = reconstruction_loss_fn(mel_recon, mel.mean(dim=2)) + \\\n",
    "                       reconstruction_loss_fn(prosody_recon, prosody.mean(dim=2)) + \\\n",
    "                       reconstruction_loss_fn(spectral_recon, spectral['mfcc'].mean(dim=2))\n",
    "                \n",
    "                contrast = contrastive_loss_fn(embedding, labels)\n",
    "                \n",
    "                loss = triplet + 0.1 * recon + 0.1 * contrast\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_triplet += triplet.item()\n",
    "                val_recon += recon.item()\n",
    "                val_contrast += contrast.item()\n",
    "        \n",
    "        # Calculate average losses\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        avg_train_triplet = train_triplet / len(train_loader)\n",
    "        avg_train_recon = train_recon / len(train_loader)\n",
    "        avg_train_contrast = train_contrast / len(train_loader)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else 0\n",
    "        avg_val_triplet = val_triplet / len(val_loader) if len(val_loader) > 0 else 0\n",
    "        avg_val_recon = val_recon / len(val_loader) if len(val_loader) > 0 else 0\n",
    "        avg_val_contrast = val_contrast / len(val_loader) if len(val_loader) > 0 else 0\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Train Triplet: {avg_train_triplet:.4f}\")\n",
    "        print(f\"Train Recon: {avg_train_recon:.4f}\")\n",
    "        print(f\"Train Contrast: {avg_train_contrast:.4f}\")\n",
    "        print(f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Val Triplet: {avg_val_triplet:.4f}\")\n",
    "        print(f\"Val Recon: {avg_val_recon:.4f}\")\n",
    "        print(f\"Val Contrast: {avg_val_contrast:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'val_loss': avg_val_loss,\n",
    "            }, 'best_accent_encoder.pt')\n",
    "        \n",
    "        # Save checkpoint every few epochs\n",
    "        if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'val_loss': avg_val_loss,\n",
    "            }, f'accent_encoder_epoch_{epoch+1}.pt')\n",
    "\n",
    "def extract_embeddings(model, dataloader, device):\n",
    "    \"\"\"Extract embeddings for all samples in the dataset\"\"\"\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    accents = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Extracting embeddings\"):\n",
    "            # Move data to device\n",
    "            mel = batch['mel'].to(device)\n",
    "            prosody = batch['prosody'].to(device)\n",
    "            \n",
    "            # Handle spectral features\n",
    "            spectral = {}\n",
    "            for key, value in batch['spectral'].items():\n",
    "                spectral[key] = value.to(device)\n",
    "            \n",
    "            # Get labels and accents\n",
    "            batch_labels = batch['label']\n",
    "            batch_accents = batch['accent']\n",
    "            \n",
    "            # Forward pass - only need embeddings\n",
    "            embedding, _, _, _ = model(mel, prosody, spectral)\n",
    "            \n",
    "            # Store embeddings and labels\n",
    "            embeddings.append(embedding.cpu())\n",
    "            labels.extend(batch_labels.tolist())\n",
    "            accents.extend(batch_accents)\n",
    "    \n",
    "    # Concatenate embeddings\n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "    \n",
    "    return embeddings, labels, accents\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    feature_root = \"/kaggle/working/extracted_data/kaggle/working/processed_features\"\n",
    "    \n",
    "    accents = {\n",
    "        \"hindi\": [\"ASI\", \"RRBI\", \"SVBI\", \"TNI\"],\n",
    "        \"spanish\": [\"EBVS\", \"ERMS\", \"MBMPS\", \"NJS\"]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        dataset = AccentEmbeddingDataset(\n",
    "            feature_root=feature_root,\n",
    "            accents=accents,\n",
    "            target_size=(128, 128)  \n",
    "        )\n",
    "        print(f\"Dataset created successfully with {len(dataset)} samples\")\n",
    "        \n",
    "        first_sample = dataset[0]\n",
    "        print(f\"First sample mel shape: {first_sample['mel'].shape}\")\n",
    "        print(f\"First sample prosody shape: {first_sample['prosody'].shape}\")\n",
    "        print(f\"First sample accent: {first_sample['accent']}\")\n",
    "        print(f\"First sample label index: {first_sample['label']}\")\n",
    "        \n",
    "        print(f\"Label to index mapping: {dataset.label_to_idx}\")\n",
    "        \n",
    "        train_size = int(0.9 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "        print(f\"Split dataset: {train_size} training samples, {val_size} validation samples\")\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=32,\n",
    "            shuffle=True,\n",
    "            num_workers=2,  \n",
    "            pin_memory=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=32,\n",
    "            shuffle=False,\n",
    "            num_workers=2, \n",
    "            pin_memory=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        model = EnhancedAccentEncoder().to(device)\n",
    "        print(\"Model created successfully\")\n",
    "        \n",
    "        print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "        train_accent_encoder(model, train_loader, val_loader, num_epochs=50, device=device)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4681447,
     "sourceId": 7958574,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
